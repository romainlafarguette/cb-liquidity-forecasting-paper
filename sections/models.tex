\section{Models}\label{sec:models}

We now give a brief description to modelling approaches that can be used for forecasting autonomous factors. As a reference to the general model classes described below see \cite{HynAth2021} and (for conditional volatility models) \citet{Tsay2005}. Throughout, $y_t$ denotes the value of an autonomous factor at time $t$, $\hat{y}_{t+h}$ denotes the $h$ day ahead forecast of an autonomous factor, made at time $t$, $m$ denotes the seasonal period (in our context $m=7$ for each day of the week) and $k$ is the integer part of $(h-1)/m$

%\subsection{Na\"ive Forecasts}

%The naive forecast is given by the last available value of the time series i.e $\hat{y}_{t+h}^{Naive}=y_{t}$. Alternatively for series with a weekly pattern (such as CIC) the last value available observed on the same day of the week of the previous period i.e., $\hat{y}_{t+h}^{SNaive}=y_{t+h-m(k+1)}$

\subsection{Exponential Smoothing}

ETS models are a class of models that employ exponential smoothing with trend and seasonal components \citep[see][for details]{HynEtAl2008}. The level, trend and seasonal components at time $t$ are given by $l_t$, $b_t$ and $s_t$ respectively. Equations for forecasts and for updating the components are given by

\begin{align}
\hat{y}_{t+h}&=l_t+hb_t+s_{t+h-m(k+1)}\\
l_t&=\alpha (y_t-s_{t-m})+(1-\alpha)(l_{t-1}+{b_{t-1}})\\
b_t&=\beta (l_t-l_{t-1})+(1-\beta)b_{t-1}\\
s_t&=\gamma (y_t-l_{t-1}-b_{t-1})+(1-\gamma)s_{t-1}\,,
\end{align}

where $\alpha$, $\beta$ and $\gamma$ are parameters that control the smoothness of the level, trend and seasonal components respectively that can be estimated by maximum likelihood. The model can be extended to allow for  multiplicative trends (with or without damping) seasonal components, and errors. Depending on the components included and whether they are additive or multiplicative, leads to 30 possible model specifications. All specifications are fit and the model with the lowest corrected Akaike Information Critrion (AICc) given by

$$-2lnL+\frac{2q(q+1)}{T-q-1}$$

where $q$  is the number of model parameters and $T$ is the size of the training sample, is chosen. The \texttt{ets} function in the \texttt{forecast} package \citep{ForPac} in the R programming environment \citep{R2020}.

\subsection{ARIMA}

The ARIMA class of models take the form

\[(1-\phi(L))(1-L)^d(y_t-\mu)=(1+\theta(L))\epsilon_t\]

where $d$ is the degree of differencing, $L$ is the lag operator, $\phi(L)=\phi_1L+\phi_2L^2+\dots+\phi_pL^p$ is a lag polynomial of AR parameters and $\theta(L)=\theta_1L+\theta_2L^2+\dots+\theta_qL^q$ is a lag polynomial of MA parameters.

The auto arima algorithm \citep{HynKha2008} can be used to select to orders $p$, $q$ and $d$. First the degree of differencing $d$ is determined by the sequqntial application of the KPSS test for stationarity. Then using the differenced data, an AR(1), AR(2) and ARMA(2,2) are fit. Starting from the model that minimises AICc, a search amongst neighboring models (i.e. models with $p$ or $q$ that differ by one from the current best model) is used to determing $p$ ad $q$.

For seasonal data, a seasonal ARIMA

\[(1-\phi(L))(1-\Phi(L^m))(1-L)^d(1-L^m)^D(y_t-\mu)=(1+\theta(L))(1-\Theta(L^m))\epsilon_t
\]

can be used instead, where $P$, $D$ and $Q$ are the orders of the seasonal AR polynomial, differencing and MA polynomial respectively. The auto arima algorithm can be modified to account for seasonality. This is implemented by the \texttt{auto.arima} function of the \texttt{forecast} package

\subsection{ARIMA with regression}

To capture the effects of holidays as well as longer seasonal patterns (e.g. yearly patterns) ARIMA models can be combined with regression. In particular, regression models are fit (possibly after differencing) with the errors then assumed to follow an ARMA process. The following regressors are considered in our modelling framework

\subsubsection{Day of Week Dummies}

Dummy variables can be used to capture weekly patterns in the data. This is particularly well suited to cases where the weekly pattern remains the same over the entire time series. It is less well suited when the seasonality varies over time, in which case seasonal ARIMA models may be more appropriate.

\subsubsection{Fourier Terms}

To capture smooth effects that take place over a longer seasonal period, Fourier terms can be used. These come in pairs and take the form
\[
x_{j,t}^{(c)}=\cos(2\pi jt/m)\textrm{  and  } x_{j,t}^{(s)}=\sin(2\pi jt/m)
\]

for $j=1,\dots,J$ and where $m$ is a longer seasonal period such as $m=365$ (days in the year). For countries with a majority Muslim population $m=354$ can be used to capture the effect of religious events (e.g. Ramadan) as well as religious holidays (e.g. Eid al Adha, Eid al Fitr) that move relative to the Gregorian calendar. In the case of the UAE, we construct Fourier terms based on four seasonal periods, $m=30$ roughly corresponding to monthly effects, $m=90$ roughly corresponding to quarterly effects, and $m=354$ and $m=365$ corresponding to the religious and civic year respectively.

\subsubsection{Holiday effects}

A standard approach to modelling the effects of holidays is to use dummy variables, which would capture a spike in an autonomous factor on a holiday. However, in the case of forecasting CIC in particular, what is more commonly observed is a ramping up of CIC for a week before a holiday, before a ramping down of CIC for a week after the holiday. This is captured by constructing the following covariate

\[
x_t^{(h)}=\max\left(\underset{t^*\in\mathcal{T}_{hol}}{\max}\left(\frac{t^*-t}{7}\right)^2,0\right)
\]

where $\mathcal{T}_{hol}$ is a set of all $t$ such that day $t$ is a public holiday. If there is a public holiday within 7 days of day $t$, the term $t^*-t$ measures the amount of days before/after a holiday. This creates a small quadratic hump around each holiday (and bears some similarities with the Prophet model \citep{TayLet2018} which uses a Gaussian density). For days that do not occur within a week of a public holiday $x_t^{(h)}=0$.

\subsubsection{Structural Breaks}

Two forms of structural breaks are often observed in autonomous factors. A transitory structural break sees a change in the level of a series for a short period. In this case dummies can be used equal to 1 for the duration of the transitory structural break and 0 otherwise. This is implemented in the case of the State Account Balance for the UAE, with a transitory break starting on March 23, 2020 and ending on April 1, 2020. Alternatively there are permanent structural breaks which see a sustained change in the level of an autonomous factor. These are modelled using dummy variables set to zero before the structural break and one after the structural break. In the case of the UAE, this was implemented for the CIC data from the March 17, 2020.

\subsubsection{Predictor Selection in Regression}

Since only a small number of groups of regressors are considered, covariates can be chosen by an exhaustive search. To clarify, blocks of day of week dummies, Fourier terms (for each seasonal frequency), holiday dummies and structural break dummies, are either included or excluded all at once, leading to $2^4=16$ model specifications. For each both non-seasonal and seasonal ARIMA errors are considered with the auto arima algorithm used to select the ARIMA orders. The model with the lowest AIC overall is then selected. An alternative to this approach would be to carry out a LASSO to conduct variable selection, or, if the blocked structure of predictors is to be preserved, then a blocked LASSO can be used instead. An advantage of an exhaustive search is that it allows the regression specification and ARIMA specification for the errors to be chosen simultaneously - although it should be noted that an exhaustive search can be computationally demanding.

\subsection{TBATS}

The TBATS model \citep{DeLEtAl2011} is a form of exponential smoothing model with a number of additional features. Seasonality evolves according to Fourier terms (similar to those used in the regression model), the series is transformed by a Box-Cox transformation (which includes the log transformation as a limiting case) and the errors of the model follow an ARMA model. The main advantage of this model is that multiple seasonal components can be used accounting for seasonality with different periodicities. This is particularly useful in countries where civic and religious calendars may differ for example, in countries with a majority Muslim population. For the UAE, TBATS is implemented with seasonal periods of $m=7,30,90,354,365$, roughly corresponding to days in a week, month, quarter, religious year and civic year.

\subsection{Conditional Heteroskedasticity models}

Net foreign assets and the aggregate series tend to resemble series on asset prices, with conditional heteroskedasticity in the difference of these series a major feature. To capture this we consider models in the GARCH family of distributions. All models for conditional volaitlity were implemented using the \texttt{rugarch} package in R~\citep{rugpac}. The standard GARCH specification \citep{Bol1986} is given by

\begin{align}
  u_t&=\sigma_t\epsilon_t\\
  \sigma_t^2&=\omega+\alpha_1u^2_{t-1}+\dots+\alpha_pu^2_{t-p}+\beta_1\sigma^2_{t-1}+\dots+\beta_q\sigma^2_{t-q}
\end{align}

where $u_t$ are demeaned changes in net foreign assets and $\epsilon_t$ is white noise with zero mean and unit variance. In modelling NFA, we consider a GARCH(1,1) specification where $p=q=1$. This model is estimated by (quasi-)maximum likelihood under the assumption that $\epsilon_t$ is normally distributed. Both stationarity and enforcing that variances are positive impose certain constraints on these parameters.

An special case of the GARCH(1,1) model that is non-stationary in given by the EWMA process 

\begin{align}
  u_t&=\sigma_t\epsilon_t\\
  \sigma_t^2&=\lambda u^2_{t-1}(1-\lambda)\sigma^2_{t-1}
\end{align}

which is a form of simple exponential smoothing for the variance rather than the mean process. An extension of the GARCH(1,1) model is the GJR-GARCH(1,1) model \citep{GloEtAl1993}

\begin{align}
  u_t&=\sigma_t\epsilon_t\\
  \sigma_t^2&=\omega+\alpha u^2_{t-1}+\gamma I(u_t>0)u^2_{t-1}\beta\sigma^2_{t-1}
\end{align}

The rationale behind the GJR-GARCH model is that the response of volatility will be stronger following a negative change in the variable rather than a positive change.

The final model we consider for conditional heteroskedasticity is the EGARCH(1,1) model \cite{Nel1991} given by

\begin{align}
  u_t&=\sigma_t\epsilon_t\\
  \ln(\sigma_t^2)&=\omega+\alpha \left(|\epsilon_{t-1}|+\gamma\epsilon_{t-1}\right)+\beta\ln(\sigma^2_{t-1})
\end{align}

An advantage of the EGARCH model is that the parameters do not need to be constrained in a way that ensures positive variances since all modelling of the variance takes place on the log scale. 

\subsection{Model Averaging}

It is well established in the forecasting literature that combining forecasts from different classes of models can improve forecast accuracy~\citep{Tim2006}. While there is a long literature on finding optimal weights, even weights tend to perform more robustly since there is no additional uncertainty introduced by having to estimate combination weights~\citep{SmiWal2009}. As well as considering an equally weighted combination of all classes of models outlined above, we also consider trimming models by only taking the best $K$ models. For this studey we consider $K=2$.%Trimming can also be achieved using model confidence sets \citep{SamSek2017} so that all models not found to perform significantly worse that the best model are used to combine forecasts.

\subsection{Forecast Reconciliation}\label{sec:forereco}

When forecasts are generated for the autonomous factors (CIC, SAB and NFA) as well as AGG, there is no guarantee that forecasts respect the aggregation relationship $AGG=NFA-(CIC+SAB)$. To address this issue, different approaches can be used. As a benchmark, consider the case of ignoring that forecasts do not aggregate correctly, this is known as a 'base' or unreconciled forecast. In the following it is convenient to stack these base forecasts in a vector

\[
\hat{\boldmath{y}}_t=\begin{pmatrix}\hat{\textrm{AGG}}_t\\
\hat{\textrm{CIC}}_t\\
\hat{\textrm{SAB}}_t\\
\hat{\textrm{NFA}}_t\end{pmatrix}
\]

The first approach to achieve coherent forecasts is to ignore the base forecast of the aggregate and instead simply compute an aggreagate forecast as 
$\hat{\textrm{NFA}}_t-(\hat{\textrm{CIC}}_t+\hat{\textrm{SAB}}_t)$. This is known as a bottom up approach. A more modern approach that does uses forecasts at all levels is forecast reconciliation.

Reconciliation methods require defining a matrix which in the case of autonomous factors is given by

\[
\mathbf{S}=\begin{pmatrix} -1 & -1 & 1\\
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{pmatrix}
\]

The first row of this matrix encodes the relationship between net liquidity and the autonomous factors. The second approach to obtain coherent forecasts is the so called OLS method of reconciliation \citep{HynEtAl2011} given by

\[
\tilde{\mathbf{y}}=\mathbf{S}\left(\mathbf{S}'\mathbf{S}\right)^{-1}\mathbf{S}'\hat{\mathbf{y}}
\]

By construction the reconciled forecasts $\tilde{\mathbf{y}}$ will cohere to the aggregation constraint.

The third way used to obtain coherent forecasts is an improvement on OLS reconciliation, known as the Minimum Trace (MinT) method\citep{WicEtAl2019}.  This method accounts for covariance in the forecasting errors of each autonomous factor. Reconciled forecasts using the the MinT method are given by

\[
\tilde{\mathbf{y}}=\mathbf{S}\left(\mathbf{S}'\boldsymbol{\Sigma}^{-1}\mathbf{S}\right)^{-1}\mathbf{S}'\boldsymbol{\Sigma}^{-1}\hat{\mathbf{y}}
\]

The covariance matrix $\boldsymbol{\Sigma}$ is estimated using the residuals from the models for each autonomous factor.

\section{Forecast Evaluation}\label{sec:eval}

Forecast evaluation is carried out via an expanding window analysis. All data begin on January 3, 2016. For the first evaluation, all models are trained using available data up to and including data on July 3, 2018. One-step to 14-step ahead forecasts are then produced, i.e. from July 4, 2018 to July 17, 2018. The second evaluation rolls the window one week ahead, i.e. using all data up to and including data on July 10, 2018, forecasts for July 11,2018 to July 24, 2018 are produced. The window is expanded one week at a time up to an origin date of December 15, 2020. This gives a total of 129 origin dates (which we denote $\mathcal{T}_{\textrm{eval}}$, with 14 forecasts at each origin date. The origin dates are all on a Tuesday - a date influence by the operational context in which the Central Bank of the UAE operates.

The $h$-step ahead forecast made at origin date $t$ are denoted $\hat{y}_{t+h|t}$. These are evaluated using RMSE and MAE given by

\[
\textrm{RMSE}_h=\sqrt{\frac{1}{|\mathcal{T}|}\sum_\mathcal{T}(y_t-\hat{y}_{t+h|t})^2}\,\textrm{and}\,\textrm{MAE}_h=\sqrt{\frac{1}{|\mathcal{T}|}\sum_\mathcal{T}|y_t-\hat{y}_{t+h|t}|}
\]

respectively. As a summary measure of forecasting performance over all horizons, squared and absolute errors can be averaged across forecasting horizons as well as forecast origins.

Given the importance of probabilistic forecasting in liquidity management we also require forecast accuracy metrics for probabilistic forecasts. A $100\times\alpha$\% prediction intervals $(\hat{l}_{t+h|t},\hat{u}_{t+h|t})$ can be evaluated by computing the coverage, that is the proportion of times the realised value of a series is does not land within the prediction interval, over the evaluation windows. 

To examine whether the differences in forecasting accuracy between different methods are statistically significant we employ the model confidence set of \citet{HanEtAl2011}. This can be interpreted in a similar fashion to a confidence interval, in the same way that a confidence interval will cover the true value of a parameter with a given probability, the model confidence set will cover the best forecasting model with a given level of probability. The model confidence sets are found using the \texttt{MCS} package in R~\citep{MCSpac} with the default settings of a 15\% Level of Signficance and 5000 bootstrap replications.

%Another way to measure the forecast accuracy of prediction intervals is the Winkler score \citep{Win1972} given by

%\[
%W^{(\alpha)}_{t+h|t}=\left\{
%\arraycolsep=1.4pt\def\arraystretch{2.2}
%\begin{array}{ll}
%\hat{u}_{t+h|t}-\hat{l}_{t+h|t}+\frac{2}{\alpha}(\hat{l}_{t+h|t}-y_{t+h})&\textrm{ if }y_{t+h}<\hat{l}_{t+h|t}\\
%\hat{u}_{t+h|t}-\hat{l}_{t+h|t}&\textrm{ if }\hat{l}_{t+h|t}\leq y_{t+h}\leq\hat{u}_{t+h|t}\\
%\hat{u}_{t+h|t}-\hat{l}_{t+h|t}+\frac{2}{\alpha}(y_{t+h|t}%-\hat{u}_{t+h})&\textrm{ if }y_{t+h}>\hat{u}_{t+h|t}
%\end{array}\right.
%\]

%These can be averaged over the evaluation windows, or over the different forecast horizons to get an overall measure of the accuracy of interval forecasts.

\section{Results}\label{sec:res}

Table~\ref{tab:cicsum} shows MAE of currency in circulation for 1-day ahead, 7-day ahead and 14-day ahead forecasts. At a 1-day horizon and 7-day horizon, the best performing methods are the model averages, however, the differences between models are quite small - the model confidence set contains all models. However at a 14-day horizon, the best performing model is the ARIMA with regression. Only this model, and the model average of the two best models are included in the model confidence set. This indicates the importance of modelling seasonality and events allowed in the regression model with ARIMA errors, especially at a two week horizon.

% latex table generated in R 4.0.3 by xtable 1.8-4 package
% Sun May 29 14:13:46 2022
\begin{table}[!ht]
\centering
\begin{tabular}{lrrr}
  \hline
Method & h=1 & h=7 & h=14 \\ 
  \hline
SES & 379 & 2121 & 3234 \\ 
  ETS & \textbf{363} & \textbf{2098} & 3188 \\ 
  ARIMA & \textbf{359} & \textbf{2012} & 2813 \\ 
  SARIMA & \textbf{378} & \textbf{2223} & 3891 \\ 
  SARIMA Reg. & \textbf{389} & \textbf{1890} & \textbf{2579} \\ 
  TBATS & \textbf{355} & \textbf{2039} & 2786 \\ 
  MA-all & \textbf{340} & \textbf{1865} & 2809 \\ 
  MA-best2 & \textbf{346} & \textbf{1854} & \textbf{2537} \\ 
   \hline
\end{tabular}
\caption{MAE at different forecast horizons (h) for currency in circulation. The methods are Simple Exponential Smoothing (SES), ETS, ARIMA, Seasonal ARIMA (SARIMA), a seasonal ARIMA with regressors for events and trigonometric seasonality (SARIMA Reg.), and two model averages, one an equally weighted average of all models (MA-all), and the other a equally weighted average of the best 2 models (MA-best2). Entries in \textbf{bold} indicate all models in the model confidence set.} 
\label{tab:cicsum}
\end{table}

Table~\ref{tab:sabsum} shows the results for State Account Balance. For this data the ARIMA models, including ARIMA with regression perform poorly at a 1-day horizon with TBATS and a model average performing best, and exponential smoothing methods within the model confidence set. At longer horizons the TBATS model performs well, although it should be noted that at a 7-day and 14-day horizon, it is difficult to distringuish between models with only simple exponential smoothing excluded from the model confidence set.

\begin{table}[!ht]
\centering
\begin{tabular}{lrrr}
  \hline
Method & h=1 & h=7 & h=14 \\ 
  \hline
Simple Exponential Smoothing & \textbf{667} & 1725 & 2010 \\ 
  ETS & \textbf{687} & \textbf{1743} & \textbf{2059} \\ 
  ARIMA & 814 & \textbf{1766} & \textbf{2025} \\ 
  Seasonal ARIMA & 803 & \textbf{1862} & \textbf{1961} \\ 
  SARIMA trig events & 829 & \textbf{1652} & \textbf{1807} \\ 
  TBATS & \textbf{731} & \textbf{1553} & \textbf{1724 }\\ 
  Model Average (all models) & \textbf{728} & \textbf{1604} & \textbf{1845} \\ 
  Model Average (best 2) & \textbf{684} & \textbf{1567} & \textbf{1801} \\ 
   \hline
\end{tabular}
\caption{MAE at different forecast horizons (h) for state account balance. The methods are Simple Exponential Smoothing (SES), ETS, ARIMA, Seasonal ARIMA (SARIMA), a seasonal ARIMA with regressors for events and trigonometric seasonality (SARIMA Reg.), and two model averages, one an equally weighted average of all models (MA-all), and the other a equally weighted average of the best 2 models (MA-best2). Entries in \textbf{bold} indicate all models in the model confidence set.} 
\label{tab:sabsum}
\end{table}

Regarding Net Foreign Assets and net liquidity due to autonomous factors, conditional volatility is the dominant feature of these series. Therefore it makes more sense to report the coverage of 95\% prediction intervals for these series. Table~\ref{tab:nfasum} reports these for net foreign assets. All models achieve coverage close to the desired rate of 95\%, with a GARCH model and an model average performing slightly better than the alternatives across all forecast horizons. Table~\ref{tab:aggsum} shows the same results for net liquidity due to autonomous factors. Again, all models achieve coverage rates close to 95\% with the model averages performing relatively well. It should be noted testing for significant differences in coverage rates will lack power due to the small number of evaluation periods in this empirical study.

\begin{table}[!ht]
\centering
\begin{tabular}{lrrr}
  \hline
Method & h=1 & h=7 & h=14 \\ 
  \hline
EWMA & 0.9250 & 0.9250 & 0.9083 \\ 
  GARCH & \textbf{0.9500} & \textbf{0.9417} & \textbf{0.9333} \\ 
  GJR-GARCH & \textbf{0.9500} & 0.9333 & 0.\textbf{9333} \\ 
  E-GARCH & 0.9333 & \textbf{0.9417} & \textbf{0.9333} \\ 
  MA-all & \textbf{0.9583} & \textbf{0.9417} & \textbf{0.9333} \\ 
  MA-best2 & 0.9333 & 0.9333 & 0.9167 \\ 
   \hline
\end{tabular}
\caption{Coverage of 95\% prediction intervals at different forecast horizons (h) for net foreign assets. The methods are Exponentially Weighted Moving Average (EWMA), GARCH, GJR-GARCH, E-GARCH (SES), and two model averages, one an equally weighted average of all models (MA-all), and the other a equally weighted average of the best 2 models (MA-best2). Entries in \textbf{bold} are those closest to 0.95.} \label{tab:nfasum}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{lrrr}
  \hline
Method & h=1 & h=7 & h=14 \\ 
  \hline
EWMA & 0.9302 & 0.9380 & 0.9302 \\ 
  GARCH & 0.9302 & \textbf{0.9535} & 0.9457 \\ 
  GJR-GARCH & \textbf{0.9380 }& \textbf{0.9535} & 0.9457 \\ 
  E-GARCH & 0.9147 & 0.9612 & 0.9380 \\ 
  MA-all &\textbf{ 0.9380} & 0.9612 & 0.9380 \\ 
  MA-best2 & \textbf{0.9380} & 0.9612 & \textbf{0.9535} \\ 
   \hline
\end{tabular}
\caption{Coverage of 95\% prediction intervals at different forecast horizons (h) for net liquidity due to autonomous factors. The methods are Exponentially Weighted Moving Average (EWMA), GARCH, GJR-GARCH, E-GARCH (SES), and two model averages, one an equally weighted average of all models (MA-all), and the other a equally weighted average of the best 2 models (MA-best2). Entries in \textbf{bold} are those closest to 0.95.} \label{tab:aggsum}
\end{table}

In addition to the forecasts from individual models, we also consider forecast reconciliation methods discussed in Section~\ref{sec:forereco}. For this evaluation we consider the regression model with ARMA errors for CIC, Simple Exponential Smoothing for SAB, an e-GARCH model for NFA and a GJR-GARCH model for AGG. Table~\ref{tab:reco} summarises the mean absolute errors for each series. The results are quite mixed, reconciliation does not guarantee an improvement in forecast accuracy for all factors at all horizons. However, there are instances where reconciliation methods lead to improved forecast accuracy. The forecast of the aggregate improves at a short horizon using the MinT method, and at a long horizon using OLS. The currency in circulation forecast can be improved at a short horizon using OLS reconciliation. Net foreign asset forecasts improve across all horizons using the OLS method. In the meantime the state account balance forecasts are not improved by using reconciliation. This result is consistent with theoretical evidence that while reconciliation can improve forecasting accuracy overall, these improvements are not guaranteed to occur for all series~\citep{PanEtAl2021}. Nonetheless, reconciliation does improve forecasts for some series/ forecasting horizons and most importantly ensures that forecasts are coherent, that is they respect the constraint AGG=NFA-(CIC+SAB).

\begin{table}[ht]
\centering
\begin{tabular}{l|l|rrr}
  \hline\hline
Autonomous Factor & Method & MAE (h=1) & MAE (h=7) & MAE (h=14) \\ 
  \hline
\multirow{4}{*}{Net Liquidity}&Base (Unreconciled) & 1760.32 & \textbf{4301.35} & 5724.03 \\ 
 & Bottom Up & 1807.91 & 4451.54 & 5902.34 \\ 
 & MinT & \textbf{1753.49} & 4397.29 & 5767.77 \\ 
  &OLS & 1768.10 & 4306.73 & \textbf{5715.94} \\ 
  \hline
\multirow{4}{*}{Currency in Circulation}  &Base (Unreconciled) & 377.69 & \textbf{1878.46} &\textbf{ 2567.23} \\ 
  &Bottom Up & 377.69 & \textbf{1878.46} & \textbf{2567.23 }\\ 
  &MinT & 375.61 & 1890.23 & 2583.85 \\ 
  &OLS & \textbf{368.58} & 1889.95 & 2584.56 \\ 
  \hline
\multirow{4}{*}{Net Foreign Assets}  &Base (Unreconciled) & 1423.02 & 4092.57 & 5643.08 \\ 
  &Bottom Up & 1423.02 & 4092.57 & 5643.08 \\ 
  &MinT & 1441.80 & 4131.70 & 5596.20 \\ 
  &OLS & \textbf{1404.91} & \textbf{3976.77} & \textbf{5500.73} \\ 
  \hline
\multirow{4}{*}{State Account Balance}  &Base (Unreconciled) & \textbf{666.86} &\textbf{ 1724.99} & \textbf{2009.55} \\ 
  &Bottom Up & \textbf{666.86} & \textbf{1724.99 }& \textbf{2009.55} \\ 
  &MinT & 742.11 & 2386.69 & 3140.85 \\ 
  &OLS & 682.95 & 1907.88 & 2284.66 \\ 
   \hline\hline
\end{tabular}
\caption{MAE for all series at different forecast horizons using different reconciliation methods.} 
\label{tab:reco}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
